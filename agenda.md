Scott's ideas for possible next steps after the course is done:

- fix some dataset issues
    - at least gemini-produced data has a lot of obviously-fake and/or insufficiently-diverse data (e.g. "Jane Doe" or "John Doe" being used as a name and being used repeatedly, phone numbers starting with 555-, "email.com" as an email domain name, etc.)
        - ?look more closely at claude and gemini data to see how widespread this is and whether there are other such problems
        - ?experiment with effects of increasing gemini generation temperature to 1.5 or 2?
        - ?try tweaking object generation prompt
- ?process the examples from the big (260ish) review file
    - need to watch out for (tweaking both json object and text passage as necessary)
        - scenario id 4: 'experience_years' in flagged-for-review records
        - scenario ids 3/13/14: the addition of "_verbatim" suffix to some field names
- another round of data generation
- update/upgrade the dataset splitting code to
    - reserve some scenarios for only validation/test or only test sets,
    - maybe include training split in addition to fewshot/validation/test
    - double check whether the split is evenly including gemini-vs-claude-generated data in each partition
    - evenly distribute a scenario's records across the partitions that that scenario is allowed to be a part of
- test out fine-tuning of gpt-4o-mini or Llama 3.3 70B
- upgrade/update the evaluation code
  - o1-mini would be very interesting and is only slightly more expensive than gpt-4o, but that would require some annoying hacks because e.g. it doesn't support the "system" role in its prompt messages and would need higher limit on output tokens than other models and so on
  - test how much of a difference it makes for the evaluation code to
    - make several queries (3? 5? 10?) and use self-consistency to assemble a final result object field by field
