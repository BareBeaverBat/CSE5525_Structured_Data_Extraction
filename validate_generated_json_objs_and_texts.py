import json
import os
import statistics
from typing import Any, Optional, Callable

import anthropic
import google.generativeai as google_genai
from anthropic import Anthropic
from google.generativeai.generative_models import safety_types, GenerativeModel

from ai_querying.ai_querying_defs import google_api_key_env, anthropic_api_key_env, google_model_specifier, \
    anthropic_model_specifier, anthropic_reconstruction_temp, google_reconstruction_temp, \
    ModelProvider, AnthropicClientBundle
from ai_querying.system_prompts import anthropic_object_reconstruction_sys_prompt, \
    google_object_reconstruction_sys_prompt
from data_processing.data_mngmt_defs import schemas_path, claude_objs_path, gemini_objs_path, claude_texts_path, \
    gemini_texts_path
from data_processing.data_loading import load_scenarios, load_objects_for_one_model_and_scenario, \
    load_text_passages_for_one_model_and_scenario
from data_processing.json_obj_comparison import evaluate_extraction
from utils_and_defs.logging_setup import create_logger
from ai_querying.ai_querying_util_funcs import extract_obj_from_passage_with_retry

logger = create_logger(__name__)

def validate_generated_objects_texts(
        google_client: Optional[GenerativeModel], anthropic_client: Optional[Anthropic], schema_idx: int, schema: dict[str,Any], scenario_domain: str,
        scenario_texts_label: str, ground_truth_objects: list[Optional[dict[str, Any]]], text_passages: list[Optional[str]],
        increment_problem_counter: Callable[[bool], None]) -> (float, dict[int, Optional[dict[str, Any]]],
                                                                dict[int, str], dict[int, float], dict[int, float], 
                                                               dict[int, int], dict[int, list[str]]):
    """
    Validates the text passages (and associated original JSON objects) generated by one model by using the other model
    to reconstruct the JSON objects from the text passages, and then comparing the reconstructed objects to the original
    :param google_client: 
    :param anthropic_client: 
    :param schema_idx: 
    :param schema: 
    :param scenario_domain: 
    :param scenario_texts_label: 
    :param ground_truth_objects: 
    :param text_passages: 
    :param increment_problem_counter: 
    :return: a tuple (where any dictionary's key is the index of the corresponding object in the ground_truth_objects
    list which failed this validation):
    - the average extraction quality (0.0 to 1.0) for the objects reconstructed from the text passages
    - a dictionary of the extracted objects that failed validation
    - a dictionary of the analyses of the extraction process for the failed extractions
    - a dictionary of the extraction quality scores for the failed extractions
    - a dictionary of the expected fact recall scores for the failed extractions
    - a dictionary of the hallucination counts for the failed extractions
    - a dictionary of the differences between original and extracted objects for the failed extractions
    """
    assert (google_client is not None) ^ (anthropic_client is not None)#XOR- exactly one of them should be defined
    was_claude_generated: bool = anthropic_client is None
    src_model_nm = "Claude" if was_claude_generated else "Gemini"
    reconstructor_model = google_model_specifier if was_claude_generated else anthropic_model_specifier
    bundled_anthropic_client = AnthropicClientBundle(anthropic_client, anthropic_object_reconstruction_sys_prompt,
                                                     4096, anthropic_reconstruction_temp, anthropic_model_specifier)
    logger.info(f"Validating {src_model_nm}-generated objects and text passages for scenario {scenario_domain} - {scenario_texts_label} with the model {reconstructor_model}")
    assert len(ground_truth_objects) == len(text_passages)
    
    val_failed_extracted_objs: dict[int, Optional[dict[str, Any]]] = {}
    val_failed_extraction_analyses: dict[int, str] = {}
    val_failed_extraction_qualities: dict[int, float] = {}
    val_failed_fact_recall: dict[int, float] = {}
    val_failed_hallucination_count: dict[int, int] = {}
    val_failed_differences: dict[int, list[str]] = {}
    
    extraction_qualities = []
    
    for obj_idx, (ground_truth_obj, passage) in enumerate(zip(ground_truth_objects, text_passages)):
        if passage is None:
            val_failed_extracted_objs[obj_idx] = None
            val_failed_extraction_analyses[obj_idx] = ("No text passage was generated for this object"
                                                       if ground_truth_obj is not None
                                                       else "object and text passage were both missing")
            val_failed_extraction_qualities[obj_idx] = 0.0
            val_failed_fact_recall[obj_idx] = 0.0
            val_failed_hallucination_count[obj_idx] = 0
            val_failed_differences[obj_idx] = []
            continue
        passage_description = (f"the {obj_idx}'th {src_model_nm}-generated text passage for scenario {schema_idx} "
                               f"{scenario_domain} - {scenario_texts_label}")
        case_id = f"{src_model_nm}-{schema_idx}-{obj_idx}"
        extracted_obj, extracted_obj_analysis = extract_obj_from_passage_with_retry(
            ModelProvider.GOOGLE_DEEPMIND if was_claude_generated else ModelProvider.ANTHROPIC, reconstructor_model,
            passage, scenario_domain, scenario_texts_label, schema, passage_description, case_id, google_client,
            bundled_anthropic_client)
        extraction_quality=0.0
        expected_fact_recall=0.0
        hallucination_count=0
        differences: list[str]=[]
        if extracted_obj is not None:            
            extraction_quality, expected_fact_recall, hallucination_count, differences = evaluate_extraction(
                ground_truth_obj, extracted_obj)
        if (1.0-extraction_quality) > 1e-8:#floating point effective-equality comparison
            logger.error(f"Extraction quality is {extraction_quality} for {src_model_nm}'s {obj_idx}th object "
                         f"for scenario {schema_idx} {scenario_domain} - {scenario_texts_label} (case id {case_id}), "
                         f"expected fact recall is {expected_fact_recall}, hallucination count is {hallucination_count}, "
                         f"and differences are {differences}\n"
                         f"Original object:\n{json.dumps(ground_truth_obj, indent=4)}\n"
                         f"Extracted object:\n{json.dumps(extracted_obj, indent=4)}\n"
                         f"Text passage:\n{text_passages[obj_idx]}\n"
                         f"Extraction analysis:\n{extracted_obj_analysis}")
            increment_problem_counter(was_claude_generated)
            val_failed_extracted_objs[obj_idx] = extracted_obj
            val_failed_extraction_analyses[obj_idx] = extracted_obj_analysis
            val_failed_extraction_qualities[obj_idx] = extraction_quality
            val_failed_fact_recall[obj_idx] = expected_fact_recall
            val_failed_hallucination_count[obj_idx] = hallucination_count
            val_failed_differences[obj_idx] = differences
        extraction_qualities.append(extraction_quality)
    assert (set(val_failed_extracted_objs.keys()) == set(val_failed_extraction_analyses.keys()) 
            == set(val_failed_extraction_qualities.keys()) == set(val_failed_fact_recall.keys()) 
            == set(val_failed_hallucination_count.keys()) == set(val_failed_differences.keys()))
    logger.info(f"Successfully extracted objects from {src_model_nm}-generated text passages for scenario {scenario_domain} - {scenario_texts_label}")
    
    return (statistics.mean(extraction_qualities), val_failed_extracted_objs, val_failed_extraction_analyses, 
            val_failed_extraction_qualities, val_failed_fact_recall, val_failed_hallucination_count, val_failed_differences)


def main():
    reconstruction_from_gemini_texts_problem_count = 0
    reconstruction_from_claude_texts_problem_count = 0
    def increment_reconstruction_problem_count(was_claude_generated_text_passage: bool):
        if was_claude_generated_text_passage:
            nonlocal reconstruction_from_claude_texts_problem_count
            reconstruction_from_claude_texts_problem_count += 1
        else:
            nonlocal reconstruction_from_gemini_texts_problem_count
            reconstruction_from_gemini_texts_problem_count += 1
    
    scenario_domains, scenario_text_passage_descriptions, schemas = load_scenarios(schemas_path)

    assert (len(scenario_domains) == len(scenario_text_passage_descriptions) == len(schemas))
    
    google_genai.configure(api_key=os.environ[google_api_key_env])
    google_generation_config={"temperature": google_reconstruction_temp, "max_output_tokens": 4096}
    
    google_client = google_genai.GenerativeModel(
        google_model_specifier, safety_settings=safety_types.HarmBlockThreshold.BLOCK_ONLY_HIGH,
        system_instruction=google_object_reconstruction_sys_prompt, generation_config=google_generation_config)
    
    anthropic_client = anthropic.Anthropic(api_key=os.environ[anthropic_api_key_env])

    start_schema_idx = 0
    schema_idx_excl_bound = len(schemas)
    logger.debug(f"Validating generated objects and text passages for scenarios {scenario_domains[start_schema_idx]} - {scenario_text_passage_descriptions[start_schema_idx]} to {scenario_domains[schema_idx_excl_bound - 1]} - {scenario_text_passage_descriptions[schema_idx_excl_bound - 1]}")
    
    extraction_qualities_for_gemini_generated_texts: dict[int, float] = {}
    extraction_qualities_for_claude_generated_texts: dict[int, float] = {}
    for was_claude_generated in [True, False]:
        for scenario_idx in range(start_schema_idx, schema_idx_excl_bound):
            schema = schemas[scenario_idx]
            scenario_domain = scenario_domains[scenario_idx]
            scenario_texts_label = scenario_text_passage_descriptions[scenario_idx]
    
            ground_truth_objects: list[dict[str, Any]] = (
                load_objects_for_one_model_and_scenario(claude_objs_path, schema, scenario_idx) if was_claude_generated
                else load_objects_for_one_model_and_scenario(gemini_objs_path, schema, scenario_idx))
            text_passages: list[str] = (
                load_text_passages_for_one_model_and_scenario(claude_texts_path, scenario_idx) if was_claude_generated
                else load_text_passages_for_one_model_and_scenario(gemini_texts_path, scenario_idx))
            
            google_client_to_use = google_client if was_claude_generated else None
            anthropic_client_to_use = None if was_claude_generated else anthropic_client
            avg_extraction_quality_for_one_models_scenario_data = validate_generated_objects_texts(
                google_client_to_use, anthropic_client_to_use, scenario_idx, schema, scenario_domain,
                scenario_texts_label, ground_truth_objects, text_passages, increment_reconstruction_problem_count)
            if was_claude_generated:
                extraction_qualities_for_claude_generated_texts[scenario_idx] = avg_extraction_quality_for_one_models_scenario_data
            else:
                extraction_qualities_for_gemini_generated_texts[scenario_idx] = avg_extraction_quality_for_one_models_scenario_data
    logger.info(f"Problems encountered with object reconstruction from text passages:\n"
                f"When Claude was extracting from Gemini-generated text passages: {reconstruction_from_gemini_texts_problem_count};\n"
                f"When Gemini was extracting from Claude-generated text passages: {reconstruction_from_claude_texts_problem_count}")
    for scenario_idx in range(start_schema_idx, schema_idx_excl_bound):
        logger.info(f"Extraction quality for scenario {scenario_domains[scenario_idx]} - {scenario_text_passage_descriptions[scenario_idx]}:\n"
                    f"from texts generated by Claude: {extraction_qualities_for_claude_generated_texts[scenario_idx]};\n"
                    f"from texts generated by Gemini: {extraction_qualities_for_gemini_generated_texts[scenario_idx]}")

if __name__ == "__main__":
    main()
